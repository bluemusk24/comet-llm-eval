{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\" width=\"250\"/>"
      ],
      "metadata": {
        "id": "O9mgL3KF9m2k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDqAkheK9j4j"
      },
      "source": [
        "# Defining a Custom Metric in Opik.\n",
        "\n",
        "In this lesson, we will define a custom metric called Factuality. You can use OpenAI or open source models via LiteLLM."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Configuration"
      ],
      "metadata": {
        "id": "5psT3jEC9e83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install comet-ml opik openai litellm --quiet"
      ],
      "metadata": {
        "id": "ZBF2E0EIyikz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0fvHKpAd9j4o",
        "outputId": "df3c2750-6414-4d71-8191-d693923c6917",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
            "* 'fields' has been removed\n",
            "  warnings.warn(message, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "from opik import Opik, track\n",
        "from opik.evaluation import evaluate\n",
        "from opik.integrations.openai import track_openai\n",
        "from opik.evaluation.metrics import base_metric, score_result\n",
        "import openai\n",
        "import os\n",
        "from datetime import datetime\n",
        "from getpass import getpass\n",
        "import litellm\n",
        "from litellm.integrations.opik.opik import OpikLogger\n",
        "from opik.opik_context import get_current_span_data\n",
        "from opik.evaluation.models import litellm_chat_model\n",
        "\n",
        "opik_logger = OpikLogger()\n",
        "# In order to log LiteLLM traces to Opik, you will need to set the Opik callback\n",
        "litellm.callbacks = [opik_logger]\n",
        "\n",
        "\n",
        "# Define project name to enable tracing\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"food_chatbot_eval\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# opik configs\n",
        "if \"OPIK_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPIK_API_KEY\"] = getpass(\"Enter your Opik API key: \")"
      ],
      "metadata": {
        "id": "JrFiw0joyD04",
        "outputId": "e1534895-627a-4eaf-bb38-1de482cdd1e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Opik API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# openai configs\n",
        "#if \"OPENAI_API_KEY\" not in os.environ:\n",
        "#    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n"
      ],
      "metadata": {
        "id": "a00iWl7XyK6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import opik\n",
        "\n",
        "opik.configure(use_local=False)"
      ],
      "metadata": {
        "id": "L7hGe7avyMWr",
        "outputId": "e50a06d4-5200-4272-81e3-040cb597e41b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Opik is already configured. You can check the settings by viewing the config file at /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Templates & Context"
      ],
      "metadata": {
        "id": "_-5O4uA5wpnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# menu items\n",
        "menu_items = \"\"\"\n",
        "Menu: Kids Menu\n",
        "Food Item: Mini Cheeseburger\n",
        "Price: $6.99\n",
        "Vegan: N\n",
        "Popularity: 4/5\n",
        "Included: Mini beef patty, cheese, lettuce, tomato, and fries.\n",
        "\n",
        "Menu: Appetizers\n",
        "Food Item: Loaded Potato Skins\n",
        "Price: $8.99\n",
        "Vegan: N\n",
        "Popularity: 3/5\n",
        "Included: Crispy potato skins filled with cheese, bacon bits, and served with sour cream.\n",
        "\n",
        "Menu: Appetizers\n",
        "Food Item: Bruschetta\n",
        "Price: $7.99\n",
        "Vegan: Y\n",
        "Popularity: 4/5\n",
        "Included: Toasted baguette slices topped with fresh tomatoes, basil, garlic, and balsamic glaze.\n",
        "\n",
        "Menu: Main Menu\n",
        "Food Item: Grilled Chicken Caesar Salad\n",
        "Price: $12.99\n",
        "Vegan: N\n",
        "Popularity: 4/5\n",
        "Included: Grilled chicken breast, romaine lettuce, Parmesan cheese, croutons, and Caesar dressing.\n",
        "\n",
        "Menu: Main Menu\n",
        "Food Item: Classic Cheese Pizza\n",
        "Price: $10.99\n",
        "Vegan: N\n",
        "Popularity: 5/5\n",
        "Included: Thin-crust pizza topped with tomato sauce, mozzarella cheese, and fresh basil.\n",
        "\n",
        "Menu: Main Menu\n",
        "Food Item: Spaghetti Bolognese\n",
        "Price: $14.99\n",
        "Vegan: N\n",
        "Popularity: 4/5\n",
        "Included: Pasta tossed in a savory meat sauce made with ground beef, tomatoes, onions, and herbs.\n",
        "\n",
        "Menu: Vegan Options\n",
        "Food Item: Veggie Wrap\n",
        "Price: $9.99\n",
        "Vegan: Y\n",
        "Popularity: 3/5\n",
        "Included: Grilled vegetables, hummus, mixed greens, and a wrap served with a side of sweet potato fries.\n",
        "\n",
        "Menu: Vegan Options\n",
        "Food Item: Vegan Beyond Burger\n",
        "Price: $11.99\n",
        "Vegan: Y\n",
        "Popularity: 4/5\n",
        "Included: Plant-based patty, vegan cheese, lettuce, tomato, onion, and a choice of regular or sweet potato fries.\n",
        "\n",
        "Menu: Desserts\n",
        "Food Item: Chocolate Lava Cake\n",
        "Price: $6.99\n",
        "Vegan: N\n",
        "Popularity: 5/5\n",
        "Included: Warm chocolate cake with a gooey molten center, served with vanilla ice cream.\n",
        "\n",
        "Menu: Desserts\n",
        "Food Item: Fresh Berry Parfait\n",
        "Price: $5.99\n",
        "Vegan: Y\n",
        "Popularity: 4/5\n",
        "Included: Layers of mixed berries, granola, and vegan coconut yogurt.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "P8vs5LU8wrvl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template for the Factuality metric\n",
        "prompt_template = \"\"\"\n",
        "###INSTRUCTIONS###\n",
        "\n",
        "You are a helpful assistant who should evaluate if a food chatbot's response is factual given user requests and a menu (delimited by +++++). Output 1 if the chatbot response is factually answering the user message and 0 if it doesn't.\n",
        "\n",
        "+++++\n",
        "{menu_items}\n",
        "+++++\n",
        "\n",
        "###EXAMPLE OUTPUT FORMAT###\n",
        "{{\n",
        "    \"value\": 0,\n",
        "    \"reason\": \"The response is not factually answering the user question.\"\n",
        "}}\n",
        "\n",
        "###INPUTS:###\n",
        "{user_message}\n",
        "\n",
        "###RESPONSE:###\n",
        "{chatbot_response}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "33y7keDlwtjo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_template = \"\"\"Answer a question about the following menu:\n",
        "\n",
        "# MENU\n",
        "{menu}\n",
        "\n",
        "# QUESTION\n",
        "{question}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mI1sS8Awwzu2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "yWxSUH-X1q7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create or get the dataset\n",
        "client = Opik()\n",
        "dataset = client.get_or_create_dataset(name=\"foodchatbot_eval\")"
      ],
      "metadata": {
        "id": "NL8CIFRl1qac"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Download Dataset From Comet"
      ],
      "metadata": {
        "id": "XdPwkTV01zgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have not previously created the `foodchatbot_eval` dataset in your Opik workspace, run the following code to download the dataset as a Comet Artifact and populate your Opik dataset.\n",
        "\n",
        "If you have already created the `foodchatbot_eval` dataset, you can skip to the next section"
      ],
      "metadata": {
        "id": "hWV6j7MM2X38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import comet_ml"
      ],
      "metadata": {
        "id": "1L8re4Wu1yDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = comet_ml.start(project_name=\"foodchatbot_eval\")\n",
        "\n",
        "logged_artifact = experiment.get_artifact(artifact_name=\"foodchatbot_eval\",\n",
        "                                        workspace=\"examples\")\n",
        "local_artifact = logged_artifact.download(\"./\")\n",
        "experiment.end()"
      ],
      "metadata": {
        "id": "5_naIp7_12on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "# Read the CSV file and insert items into the dataset\n",
        "with open('./foodchatbot_clean_eval_dataset.csv', newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "        index, question, response = row\n",
        "        item = {\n",
        "            \"index\": index,\n",
        "            \"question\": question,\n",
        "            \"response\": response\n",
        "        }\n",
        "\n",
        "        dataset.insert([item])"
      ],
      "metadata": {
        "id": "NtuhLA0X12mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Your Application --> if using OpenAI"
      ],
      "metadata": {
        "id": "3nmTICPOw2XV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple little client class for using different LLM APIs (OpenAI or LiteLLM)\n",
        "class LLMClient:\n",
        "  def __init__(self, client_type: str =\"openai\", model: str =\"gpt-4o-mini\"):\n",
        "    self.client_type = client_type\n",
        "    self.model = model\n",
        "\n",
        "    if self.client_type == \"openai\":\n",
        "      self.client = track_openai(openai.OpenAI())\n",
        "\n",
        "    else:\n",
        "      self.client = None\n",
        "\n",
        "  # LiteLLM query function\n",
        "  def _get_litellm_response(self, query: str, system: str = \"You are a helpful assistant.\"):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system },\n",
        "        { \"role\": \"user\", \"content\": query }\n",
        "    ]\n",
        "\n",
        "    response = litellm.completion(\n",
        "        model=self.model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "  # OpenAI query function - use **kwargs to pass arguments like temperature\n",
        "  def _get_openai_response(self, query: str, system: str = \"You are a helpful assistant.\", **kwargs):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system },\n",
        "        { \"role\": \"user\", \"content\": query }\n",
        "    ]\n",
        "\n",
        "    response = self.client.chat.completions.create(\n",
        "        model=self.model,\n",
        "        messages=messages,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "  def query(self, query: str, system: str = \"You are a helpful assistant.\", **kwargs):\n",
        "    if self.client_type == 'openai':\n",
        "      return self._get_openai_response(query, system, **kwargs)\n",
        "\n",
        "    else:\n",
        "      return self._get_litellm_response(query, system)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bZ1hCID4vaAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your client!\n",
        "\n",
        "llm_client = LLMClient()"
      ],
      "metadata": {
        "id": "aiLj5YNUyTXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zgUgIEjz6yGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation  --> using OpenAI"
      ],
      "metadata": {
        "id": "9a9UlyKZ9ZNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Factuality Metric\n",
        "class Factuality(base_metric.BaseMetric):\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "\n",
        "    def score(self, input: str, output: str, context: str, reference: str):\n",
        "        response = llm_client.query(prompt_template.format(menu_items=context, user_message=input, chatbot_response=output))\n",
        "\n",
        "        response = eval(response)\n",
        "\n",
        "        return score_result.ScoreResult(\n",
        "            value=response[\"value\"],\n",
        "            name=self.name,\n",
        "            reason=response[\"reason\"]\n",
        "        )\n"
      ],
      "metadata": {
        "id": "vwmKvx8_vT2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dI-rxB3iwSlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MJp8lQP9j4r"
      },
      "outputs": [],
      "source": [
        "@track\n",
        "def chatbot_application(input: str) -> str:\n",
        "    response = llm_client.query(question_template.format(menu=menu_items, question=input))\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation task\n",
        "def evaluation_task(x: DatasetItem):\n",
        "    return {\n",
        "        \"input\": x['question'],\n",
        "        \"output\": chatbot_application(x['question']),\n",
        "        \"context\": menu_items,\n",
        "        \"reference\": x['response']\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Gvj3h1CixJ2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Opik()"
      ],
      "metadata": {
        "id": "EtYYa2La2mzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the metrics\n",
        "metrics = [Factuality(\"Factuality\")]"
      ],
      "metadata": {
        "id": "WU5yRdaQxKo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation\n",
        "experiment_name = \"gpt-4o-mini\" + \"_\" + dataset.name + \"_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "evaluation = evaluate(\n",
        "    experiment_name=experiment_name,\n",
        "    dataset=dataset,\n",
        "    task=evaluation_task,\n",
        "    scoring_metrics=metrics,\n",
        "    experiment_config={\n",
        "        \"model\": \"gpt-4o-mini\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "ebvqkbRDxKly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdUvk7lvxKiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Application with LiteLLM llama3.2 and Evaluate"
      ],
      "metadata": {
        "id": "MIiNCEBe6kM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Configs to access meta-llama-3.2 model\n",
        "if \"HF_TOKEN\" not in os.environ:\n",
        "  os.environ[\"HF_TOKEN\"] = getpass(\"Enter your Hugging Face Key: \")"
      ],
      "metadata": {
        "id": "kDbHEurE6-Kj",
        "outputId": "1365f15e-2b66-48cb-f6b6-c91aaab5ba36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# meta-llama from HuggingFace\n",
        "MODEL = \"huggingface/meta-llama/Llama-3.2-3B-Instruct\""
      ],
      "metadata": {
        "id": "WNWokLsl7PKq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Factuality Metric\n",
        "class Factuality(base_metric.BaseMetric):\n",
        "    def __init__(self, name: str, model: str = \"huggingface/meta-llama/Llama-3.2-3B-Instruct\"):\n",
        "        self.name = name\n",
        "        self.llm_client = litellm_chat_model.LiteLLMChatModel(model_name=model)\n",
        "\n",
        "    def score(self, input: str, output: str, context: str, reference: str = None, **kwargs):\n",
        "        # Generate response from the LLM\n",
        "        response = self.llm_client.generate_string(prompt_template.format(menu_items=context, user_message=input, chatbot_response=output))\n",
        "        response = eval(response)\n",
        "\n",
        "        return score_result.ScoreResult(\n",
        "            value=response[\"value\"],\n",
        "            name=self.name,\n",
        "            reason=response[\"reason\"]\n",
        "        )\n"
      ],
      "metadata": {
        "id": "dEEHLmOtOWxq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@track\n",
        "def chatbot_application(input: str) -> str:\n",
        "    response = litellm.completion(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "            {\"role\":\"user\", \"content\":question_template.format(menu=menu_items, question=input)}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "s0q7Ii6mEkkx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation task\n",
        "def evaluation_task(x):                       # (x: DatasetItem):\n",
        "    return {\n",
        "        \"input\": x['question'],\n",
        "        \"output\": chatbot_application(x['question']),\n",
        "        \"context\": menu_items,\n",
        "        \"reference\": x['response']\n",
        "    }"
      ],
      "metadata": {
        "id": "-Phphbp1E7F-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the metrics\n",
        "metrics = [Factuality(\"Factuality\")]"
      ],
      "metadata": {
        "id": "TAGkrNWLFQfV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation\n",
        "experiment_name = MODEL + \"_\" + dataset.name + \"_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "evaluation = evaluate(\n",
        "    experiment_name=experiment_name,\n",
        "    dataset=dataset,\n",
        "    task=evaluation_task,\n",
        "    scoring_metrics=metrics,\n",
        "    experiment_config={\n",
        "        \"model\": MODEL\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "8PwQpPU9FnWQ",
        "outputId": "cf1d4138-6d4b-49c3-eba6-9687216d5c80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation:  19%|█▉        | 11/57 [00:02<00:04,  9.93it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    Is the Vegan Beyond Burger a popular choice?\n",
            "       ^^^\n",
            "SyntaxError: invalid syntax\n",
            "Evaluation:  26%|██▋       | 15/57 [00:02<00:04,  8.53it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    The response claims that there is only 1 kind of pizza on the menu, which is the Classic Cheese Pizza. However, we need to check if this is true.\n",
            "        ^^^^^^^^\n",
            "SyntaxError: invalid syntax\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 5\n",
            "    The response does not provide any information about the prep time for the dishes on the menu, which is the user's question.\n",
            "                                                                                                                   ^\n",
            "SyntaxError: unterminated string literal (detected at line 5)\n",
            "Evaluation:  40%|████      | 23/57 [00:03<00:03,  9.38it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    Is the Veggie Wrap vegan?\n",
            "       ^^^\n",
            "SyntaxError: invalid syntax\n",
            "Evaluation:  44%|████▍     | 25/57 [00:03<00:03,  9.25it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    Is the Spaghetti Bolognese made with vegetarian ingredients?\n",
            "       ^^^\n",
            "SyntaxError: invalid syntax\n",
            "Evaluation:  54%|█████▍    | 31/57 [00:04<00:02,  9.50it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    The chatbot's response states that there are no seasonal dishes listed on the menu.\n",
            "               ^\n",
            "SyntaxError: unterminated string literal (detected at line 2)\n",
            "Evaluation:  58%|█████▊    | 33/57 [00:04<00:02,  9.47it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    How many vegan options are available on the menu?\n",
            "        ^^^^\n",
            "SyntaxError: invalid syntax\n",
            "Evaluation:  61%|██████▏   | 35/57 [00:05<00:02,  8.52it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    Is there a dish that features grilled chicken?\n",
            "       ^^^^^\n",
            "SyntaxError: invalid syntax\n",
            "Evaluation:  65%|██████▍   | 37/57 [00:05<00:02,  8.69it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    What is the price of the Mini Cheeseburger?\n",
            "                ^^^^^\n",
            "SyntaxError: invalid syntax\n",
            "Evaluation:  72%|███████▏  | 41/57 [00:05<00:01, 11.10it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    The relevant menu section for desserts is the \"Desserts\" section.\n",
            "        ^^^^^^^^\n",
            "SyntaxError: invalid syntax\n",
            "Evaluation:  75%|███████▌  | 43/57 [00:05<00:01, 12.68it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    Are the fries served with the Mini Cheeseburger regular or sweet potato?\n",
            "        ^^^\n",
            "SyntaxError: invalid syntax\n",
            "Evaluation:  79%|███████▉  | 45/57 [00:05<00:00, 13.53it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    Is the Chocolate Lava Cake suitable for someone with a lactose intolerance?\n",
            "       ^^^\n",
            "SyntaxError: invalid syntax\n",
            "Evaluation:  82%|████████▏ | 47/57 [00:05<00:00, 12.22it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    What is the price of the Mini Cheeseburger?\n",
            "                ^^^^^\n",
            "SyntaxError: invalid syntax\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    Is the Vegan Beyond Burger gluten-free?\n",
            "       ^^^\n",
            "SyntaxError: invalid syntax\n",
            "Evaluation:  91%|█████████ | 52/57 [00:06<00:00, 12.94it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    Is the Classic Cheese Pizza suitable for vegans?\n",
            "       ^^^\n",
            "SyntaxError: invalid syntax\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    How much does the Vegan Beyond Burger cost?\n",
            "        ^^^^\n",
            "SyntaxError: invalid syntax\n",
            "Evaluation:  95%|█████████▍| 54/57 [00:06<00:00, 11.88it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    What is the price of the Mini Cheeseburger?\n",
            "                ^^^^^\n",
            "SyntaxError: invalid syntax\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 2\n",
            "    The chatbot's response directly answers the user's question about the price of the Loaded Potato Skins.\n",
            "        ^^^^^^^\n",
            "SyntaxError: invalid syntax\n",
            "Evaluation:  98%|█████████▊| 56/57 [00:07<00:00,  7.65it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"<ipython-input-21-a4dc4d035c48>\", line 10, in score\n",
            "    response = eval(response)\n",
            "  File \"<string>\", line 5\n",
            "    Since the user's question is not provided, I will wait for the user to input a question.\n",
            "                  ^\n",
            "SyntaxError: unterminated string literal (detected at line 5)\n",
            "Evaluation: 100%|██████████| 57/57 [00:07<00:00,  7.97it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ foodchatbot_eval (57 samples) ──────╮\n",
              "│                                      │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:00:07          │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 57                │\n",
              "│                                      │\n",
              "│ \u001b[1;32mFactuality: 0.7105 (avg)\u001b[0m\u001b[31m - 19 failed\u001b[0m │\n",
              "│                                      │\n",
              "╰──────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ foodchatbot_eval (57 samples) ──────╮\n",
              "│                                      │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:00:07          │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 57                │\n",
              "│                                      │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Factuality: 0.7105 (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 19 failed</span> │\n",
              "│                                      │\n",
              "╰──────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=916870;https://www.comet.com/opik/bluemusk/experiments/01944956-b0bd-7ee4-be47-fdc2c394ea4f/compare?experiments=%5B%2201944d61-7ff1-7ef3-9dd9-69fe8f319e6f%22%5D\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/bluemusk/experiments/01944956-b0bd-7ee4-be47-fdc2c394ea4f/compare?experiments=%5B%2201944d61-7ff1-7ef3-9dd9-69fe8f319e6f%22%5D\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MLeIyCIUYX-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "comet-eval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5psT3jEC9e83",
        "_-5O4uA5wpnU",
        "yWxSUH-X1q7x",
        "3nmTICPOw2XV",
        "9a9UlyKZ9ZNl"
      ],
      "name": "custom-metric-evaluation.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}