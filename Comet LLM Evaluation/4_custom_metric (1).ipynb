{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\" width=\"250\"/>"
      ],
      "metadata": {
        "id": "O9mgL3KF9m2k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDqAkheK9j4j"
      },
      "source": [
        "# Defining a Custom Metric in Opik.\n",
        "\n",
        "In this lesson, we will define a custom metric called Factuality. You can use OpenAI or open source models via LiteLLM."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Configuration"
      ],
      "metadata": {
        "id": "5psT3jEC9e83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install comet-ml opik openai litellm --quiet"
      ],
      "metadata": {
        "id": "ZBF2E0EIyikz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0fvHKpAd9j4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac79d2f5-11f1-488d-8c0b-62615f4b25ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
            "* 'fields' has been removed\n",
            "  warnings.warn(message, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "from opik import Opik, track\n",
        "from opik.evaluation import evaluate\n",
        "from opik.integrations.openai import track_openai\n",
        "from opik.evaluation.metrics import base_metric, score_result\n",
        "import openai\n",
        "import os\n",
        "from datetime import datetime\n",
        "from getpass import getpass\n",
        "import litellm\n",
        "from litellm.integrations.opik.opik import OpikLogger\n",
        "from opik.opik_context import get_current_span_data\n",
        "\n",
        "opik_logger = OpikLogger()\n",
        "# In order to log LiteLLM traces to Opik, you will need to set the Opik callback\n",
        "litellm.callbacks = [opik_logger]\n",
        "\n",
        "\n",
        "# Define project name to enable tracing\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"food_chatbot_eval\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# opik configs\n",
        "if \"OPIK_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPIK_API_KEY\"] = getpass(\"Enter your Opik API key: \")\n",
        "\n"
      ],
      "metadata": {
        "id": "JrFiw0joyD04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29dde695-926b-4aaa-acd8-98692da8cba5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Opik API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# openai configs\n",
        "#if \"OPENAI_API_KEY\" not in os.environ:\n",
        "#    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n"
      ],
      "metadata": {
        "id": "a00iWl7XyK6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import opik\n",
        "\n",
        "opik.configure(use_local=False)"
      ],
      "metadata": {
        "id": "L7hGe7avyMWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a71798-9b51-4a19-c224-399626d31651"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Opik is already configured. You can check the settings by viewing the config file at /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Templates & Context"
      ],
      "metadata": {
        "id": "_-5O4uA5wpnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# menu items\n",
        "menu_items = \"\"\"\n",
        "Menu: Kids Menu\n",
        "Food Item: Mini Cheeseburger\n",
        "Price: $6.99\n",
        "Vegan: N\n",
        "Popularity: 4/5\n",
        "Included: Mini beef patty, cheese, lettuce, tomato, and fries.\n",
        "\n",
        "Menu: Appetizers\n",
        "Food Item: Loaded Potato Skins\n",
        "Price: $8.99\n",
        "Vegan: N\n",
        "Popularity: 3/5\n",
        "Included: Crispy potato skins filled with cheese, bacon bits, and served with sour cream.\n",
        "\n",
        "Menu: Appetizers\n",
        "Food Item: Bruschetta\n",
        "Price: $7.99\n",
        "Vegan: Y\n",
        "Popularity: 4/5\n",
        "Included: Toasted baguette slices topped with fresh tomatoes, basil, garlic, and balsamic glaze.\n",
        "\n",
        "Menu: Main Menu\n",
        "Food Item: Grilled Chicken Caesar Salad\n",
        "Price: $12.99\n",
        "Vegan: N\n",
        "Popularity: 4/5\n",
        "Included: Grilled chicken breast, romaine lettuce, Parmesan cheese, croutons, and Caesar dressing.\n",
        "\n",
        "Menu: Main Menu\n",
        "Food Item: Classic Cheese Pizza\n",
        "Price: $10.99\n",
        "Vegan: N\n",
        "Popularity: 5/5\n",
        "Included: Thin-crust pizza topped with tomato sauce, mozzarella cheese, and fresh basil.\n",
        "\n",
        "Menu: Main Menu\n",
        "Food Item: Spaghetti Bolognese\n",
        "Price: $14.99\n",
        "Vegan: N\n",
        "Popularity: 4/5\n",
        "Included: Pasta tossed in a savory meat sauce made with ground beef, tomatoes, onions, and herbs.\n",
        "\n",
        "Menu: Vegan Options\n",
        "Food Item: Veggie Wrap\n",
        "Price: $9.99\n",
        "Vegan: Y\n",
        "Popularity: 3/5\n",
        "Included: Grilled vegetables, hummus, mixed greens, and a wrap served with a side of sweet potato fries.\n",
        "\n",
        "Menu: Vegan Options\n",
        "Food Item: Vegan Beyond Burger\n",
        "Price: $11.99\n",
        "Vegan: Y\n",
        "Popularity: 4/5\n",
        "Included: Plant-based patty, vegan cheese, lettuce, tomato, onion, and a choice of regular or sweet potato fries.\n",
        "\n",
        "Menu: Desserts\n",
        "Food Item: Chocolate Lava Cake\n",
        "Price: $6.99\n",
        "Vegan: N\n",
        "Popularity: 5/5\n",
        "Included: Warm chocolate cake with a gooey molten center, served with vanilla ice cream.\n",
        "\n",
        "Menu: Desserts\n",
        "Food Item: Fresh Berry Parfait\n",
        "Price: $5.99\n",
        "Vegan: Y\n",
        "Popularity: 4/5\n",
        "Included: Layers of mixed berries, granola, and vegan coconut yogurt.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "P8vs5LU8wrvl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template for the Factuality metric\n",
        "prompt_template = \"\"\"\n",
        "###INSTRUCTIONS###\n",
        "\n",
        "You are a helpful assistant who should evaluate if a food chatbot's response is factual given user requests and a menu (delimited by +++++). Output 1 if the chatbot response is factually answering the user message and 0 if it doesn't.\n",
        "\n",
        "+++++\n",
        "{menu_items}\n",
        "+++++\n",
        "\n",
        "###EXAMPLE OUTPUT FORMAT###\n",
        "{{\n",
        "    \"value\": 0,\n",
        "    \"reason\": \"The response is not factually answering the user question.\"\n",
        "}}\n",
        "\n",
        "###INPUTS:###\n",
        "{user_message}\n",
        "\n",
        "###RESPONSE:###\n",
        "{chatbot_response}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "33y7keDlwtjo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_template = \"\"\"Answer a question about the following menu:\n",
        "\n",
        "# MENU\n",
        "{menu}\n",
        "\n",
        "# QUESTION\n",
        "{question}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mI1sS8Awwzu2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "yWxSUH-X1q7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create or get the dataset\n",
        "client = Opik()\n",
        "dataset = client.get_or_create_dataset(name=\"foodchatbot_eval\")"
      ],
      "metadata": {
        "id": "NL8CIFRl1qac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aefb26e-7e51-445e-db6b-ae9f24881219"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Created a \"foodchatbot_eval\" dataset at https://www.comet.com/opik/bluemusk/redirect/datasets?name=foodchatbot_eval.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Download Dataset From Comet"
      ],
      "metadata": {
        "id": "XdPwkTV01zgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have not previously created the `foodchatbot_eval` dataset in your Opik workspace, run the following code to download the dataset as a Comet Artifact and populate your Opik dataset.\n",
        "\n",
        "If you have already created the `foodchatbot_eval` dataset, you can skip to the next section"
      ],
      "metadata": {
        "id": "hWV6j7MM2X38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import comet_ml"
      ],
      "metadata": {
        "id": "1L8re4Wu1yDv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comet_ml.login(api_key=os.environ[\"OPIK_API_KEY\"])\n",
        "experiment = comet_ml.start(project_name=\"foodchatbot_eval\")\n",
        "\n",
        "logged_artifact = experiment.get_artifact(artifact_name=\"foodchatbot_eval\",\n",
        "                                          workspace=\"examples\")\n",
        "local_artifact = logged_artifact.download(\"./\")\n",
        "experiment.end()"
      ],
      "metadata": {
        "id": "5_naIp7_12on",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562a998d-f96c-4d0d-d73d-115aa92b35a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Valid Comet API Key saved in /root/.comet.config (set COMET_CONFIG to change where it is saved).\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/bluemusk/foodchatbot-eval/f8a79d4151ec4585b3b3be705da207cc\n",
            "\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Artifact 'examples/foodchatbot_eval:2.0.0' download has been started asynchronously\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still downloading 1 file(s), remaining 7.54 KB/7.54 KB\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Artifact 'examples/foodchatbot_eval:2.0.0' has been successfully downloaded\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : continuous_mason_9571\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/bluemusk/foodchatbot-eval/f8a79d4151ec4585b3b3be705da207cc\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Downloads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     artifact assets : 1 (7.54 KB)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     artifacts       : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook_url : https://colab.research.google.com/notebook#fileId=https%3A%2F%2Fgist.github.com%2Fcaleb-kaiser%2F1df38e5268f80f4fc46316a0cbba2f39%23file-4-custom-metric-ipynb\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "# Read the CSV file and insert items into the dataset\n",
        "with open('./foodchatbot_clean_eval_dataset.csv', newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "        index, question, response = row\n",
        "        item = {\n",
        "            \"index\": index,\n",
        "            \"question\": question,\n",
        "            \"response\": response\n",
        "        }\n",
        "\n",
        "        dataset.insert([item])"
      ],
      "metadata": {
        "id": "NtuhLA0X12mN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Your Application"
      ],
      "metadata": {
        "id": "3nmTICPOw2XV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple little client class for using different LLM APIs (OpenAI or LiteLLM)\n",
        "class LLMClient:\n",
        "  def __init__(self, client_type: str =\"openai\", model: str =\"gpt-4o-mini\"):\n",
        "    self.client_type = client_type\n",
        "    self.model = model\n",
        "\n",
        "    if self.client_type == \"openai\":\n",
        "      self.client = track_openai(openai.OpenAI())\n",
        "\n",
        "    else:\n",
        "      self.client = None\n",
        "\n",
        "  # LiteLLM query function\n",
        "  def _get_litellm_response(self, query: str, system: str = \"You are a helpful assistant.\"):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system },\n",
        "        { \"role\": \"user\", \"content\": query }\n",
        "    ]\n",
        "\n",
        "    response = litellm.completion(\n",
        "        model=self.model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "  # OpenAI query function - use **kwargs to pass arguments like temperature\n",
        "  def _get_openai_response(self, query: str, system: str = \"You are a helpful assistant.\", **kwargs):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system },\n",
        "        { \"role\": \"user\", \"content\": query }\n",
        "    ]\n",
        "\n",
        "    response = self.client.chat.completions.create(\n",
        "        model=self.model,\n",
        "        messages=messages,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "  def query(self, query: str, system: str = \"You are a helpful assistant.\", **kwargs):\n",
        "    if self.client_type == 'openai':\n",
        "      return self._get_openai_response(query, system, **kwargs)\n",
        "\n",
        "    else:\n",
        "      return self._get_litellm_response(query, system)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bZ1hCID4vaAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your client!\n",
        "\n",
        "llm_client = LLMClient()"
      ],
      "metadata": {
        "id": "aiLj5YNUyTXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Application using LiteLLm and Evaluate"
      ],
      "metadata": {
        "id": "BepjTqBBW2KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Configs to access meta-llama-3.2 model\n",
        "if \"HF_TOKEN\" not in os.environ:\n",
        "  os.environ[\"HF_TOKEN\"] = getpass(\"Enter your Hugging Face Key: \")"
      ],
      "metadata": {
        "id": "Q1u-_yRcXAHp",
        "outputId": "6354c5bd-bdcc-4a0c-f0bf-2efff2f72aa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# meta-llama from HuggingFace\n",
        "MODEL = \"huggingface/meta-llama/Llama-3.2-3B-Instruct\""
      ],
      "metadata": {
        "id": "ajWNg5DuXXue"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Factuality Metric\n",
        "class Factuality(base_metric.BaseMetric):\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "\n",
        "    def score(self, input: str, output: str, context: str, reference: str):\n",
        "        response = litellm.completion(\n",
        "            model=MODEL,\n",
        "            messages=[\n",
        "                {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "                {\"role\":\"user\", \"content\":prompt_template.format(menu_items=context, user_message=input, chatbot_response=output)}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        response = eval(response)\n",
        "\n",
        "        return score_result.ScoreResult(\n",
        "            value=response[\"value\"],\n",
        "            name=self.name,\n",
        "            reason=response[\"reason\"]\n",
        "        )"
      ],
      "metadata": {
        "id": "hno2ck44XoLG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@track\n",
        "def chatbot_application(input: str) -> str:\n",
        "    response = litellm.completion(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "            {\"role\":\"user\", \"content\":question_template.format(menu=menu_items, question=input)}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "BV-wsaXzYPQF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation task\n",
        "def evaluation_task(x):                       # (x: DatasetItem):\n",
        "    return {\n",
        "        \"input\": x['question'],\n",
        "        \"output\": chatbot_application(x['question']),\n",
        "        \"context\": menu_items,\n",
        "        \"reference\": x['response']\n",
        "    }"
      ],
      "metadata": {
        "id": "wlgMU0IfcTX4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the metrics\n",
        "metrics = [Factuality(\"Factuality\")]"
      ],
      "metadata": {
        "id": "QSynPvJTcqMF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation\n",
        "experiment_name = MODEL + \"_\" + dataset.name + \"_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "evaluation = evaluate(\n",
        "    experiment_name=experiment_name,\n",
        "    dataset=dataset,\n",
        "    task=evaluation_task,\n",
        "    scoring_metrics=metrics,\n",
        "    experiment_config={\n",
        "        \"model\": MODEL\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "uS05nXBQctjj",
        "outputId": "00958942-322e-4227-dd3d-f6043462aa5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/57 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Started logging traces to the \"food_chatbot_eval\" project at https://www.comet.com/opik/bluemusk/redirect/projects?name=food_chatbot_eval.\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:   2%|▏         | 1/57 [00:12<11:41, 12.53s/it]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:   4%|▎         | 2/57 [00:12<04:48,  5.25s/it]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  23%|██▎       | 13/57 [00:12<00:17,  2.46it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  32%|███▏      | 18/57 [00:13<00:10,  3.60it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  39%|███▊      | 22/57 [00:13<00:07,  4.94it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  46%|████▌     | 26/57 [00:13<00:04,  6.46it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  51%|█████     | 29/57 [00:13<00:03,  7.54it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  56%|█████▌    | 32/57 [00:14<00:02,  8.55it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  61%|██████▏   | 35/57 [00:14<00:02, 10.31it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  67%|██████▋   | 38/57 [00:14<00:01, 11.43it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  70%|███████   | 40/57 [00:14<00:01, 10.35it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  79%|███████▉  | 45/57 [00:14<00:00, 15.36it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  84%|████████▍ | 48/57 [00:14<00:00, 15.70it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  91%|█████████ | 52/57 [00:15<00:00, 17.80it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation:  96%|█████████▋| 55/57 [00:15<00:00, 15.89it/s]OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "OPIK: Failed to compute metric Factuality. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "TypeError: Factuality.score() got an unexpected keyword argument 'question'\n",
            "Evaluation: 100%|██████████| 57/57 [00:15<00:00,  3.62it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ foodchatbot_eval (57 samples) ────╮\n",
              "│                                    │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:00:16        │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 57              │\n",
              "│                                    │\n",
              "│ \u001b[1;32mFactuality: None (avg)\u001b[0m\u001b[31m - 57 failed\u001b[0m │\n",
              "│                                    │\n",
              "╰────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ foodchatbot_eval (57 samples) ────╮\n",
              "│                                    │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:00:16        │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 57              │\n",
              "│                                    │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Factuality: None (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 57 failed</span> │\n",
              "│                                    │\n",
              "╰────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=81773;https://www.comet.com/opik/bluemusk/experiments/01944956-b0bd-7ee4-be47-fdc2c394ea4f/compare?experiments=%5B%2201944974-9243-7052-889a-563fd2199faa%22%5D\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/bluemusk/experiments/01944956-b0bd-7ee4-be47-fdc2c394ea4f/compare?experiments=%5B%2201944974-9243-7052-889a-563fd2199faa%22%5D\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation using OpenAI\n",
        "\n",
        "* Not necessary cos I'm using Litellm above"
      ],
      "metadata": {
        "id": "9a9UlyKZ9ZNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Factuality Metric\n",
        "class Factuality(base_metric.BaseMetric):\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "\n",
        "    def score(self, input: str, output: str, context: str, reference: str):\n",
        "        response = llm_client.query(prompt_template.format(menu_items=context, user_message=input, chatbot_response=output))\n",
        "\n",
        "        response = eval(response)\n",
        "\n",
        "        return score_result.ScoreResult(\n",
        "            value=response[\"value\"],\n",
        "            name=self.name,\n",
        "            reason=response[\"reason\"]\n",
        "        )\n"
      ],
      "metadata": {
        "id": "vwmKvx8_vT2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dI-rxB3iwSlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MJp8lQP9j4r"
      },
      "outputs": [],
      "source": [
        "@track\n",
        "def chatbot_application(input: str) -> str:\n",
        "    response = llm_client.query(question_template.format(menu=menu_items, question=input))\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation task\n",
        "def evaluation_task(x: DatasetItem):\n",
        "    return {\n",
        "        \"input\": x['question'],\n",
        "        \"output\": chatbot_application(x['question']),\n",
        "        \"context\": menu_items,\n",
        "        \"reference\": x['response']\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Gvj3h1CixJ2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Opik()"
      ],
      "metadata": {
        "id": "EtYYa2La2mzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the metrics\n",
        "metrics = [Factuality(\"Factuality\")]"
      ],
      "metadata": {
        "id": "WU5yRdaQxKo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run evaluation\n",
        "experiment_name = \"gpt-4o-mini\" + \"_\" + dataset.name + \"_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "evaluation = evaluate(\n",
        "    experiment_name=experiment_name,\n",
        "    dataset=dataset,\n",
        "    task=evaluation_task,\n",
        "    scoring_metrics=metrics,\n",
        "    experiment_config={\n",
        "        \"model\": \"gpt-4o-mini\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "ebvqkbRDxKly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdUvk7lvxKiK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "comet-eval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5psT3jEC9e83",
        "_-5O4uA5wpnU",
        "yWxSUH-X1q7x",
        "3nmTICPOw2XV",
        "9a9UlyKZ9ZNl"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}