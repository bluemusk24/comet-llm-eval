{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\" width=\"250\"/>"
      ],
      "metadata": {
        "id": "VyT73jSw8nQJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tnPHsVuzY2O"
      },
      "source": [
        "# LLM-Based Evaluation with Opik"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you'll be evaluationg LLM applications with LLM-as-a-judge metrics. You can use OpenAI or open source models via LiteLLM. To make the exercise a little more exciting, you'll be running your evaluations using HaluBench, the popular hallucination dataset."
      ],
      "metadata": {
        "id": "oCCyTFCia3A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Configuration"
      ],
      "metadata": {
        "id": "rkMMwPik0obY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install opik openai comet_ml litellm --quiet"
      ],
      "metadata": {
        "id": "jqGjzWRc0k0n",
        "outputId": "1170aaf0-f74c-451d-d822-c99adec86eef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/303.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.5/303.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m710.6/710.6 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m980.3/980.3 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MfaBKoVnzY2R",
        "outputId": "7c5bfda3-ffb0-4bbe-8021-46ae75a16a74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
            "* 'fields' has been removed\n",
            "  warnings.warn(message, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "import opik\n",
        "from opik import Opik, track\n",
        "from opik.evaluation import evaluate\n",
        "from opik.evaluation.metrics import (Hallucination, AnswerRelevance)\n",
        "from opik.integrations.openai import track_openai\n",
        "import openai\n",
        "import os\n",
        "from datetime import datetime\n",
        "from getpass import getpass\n",
        "import litellm\n",
        "from litellm.integrations.opik.opik import OpikLogger\n",
        "from opik.opik_context import get_current_span_data\n",
        "\n",
        "opik_logger = OpikLogger()\n",
        "# In order to log LiteLLM traces to Opik, you will need to set the Opik callback\n",
        "litellm.callbacks = [opik_logger]\n",
        "\n",
        "# Define project name to enable tracing\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"llm-based-eval\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# opik configs\n",
        "if \"OPIK_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPIK_API_KEY\"] = getpass(\"Enter your Opik API key: \")\n",
        "\n",
        "opik.configure()"
      ],
      "metadata": {
        "id": "BNDJe4iZ1Ogd",
        "outputId": "7fbacf54-6ea1-417a-a02f-e4cbf1b229fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Opik API key: ··········\n",
            "Do you want to use \"bluemusk\" workspace? (Y/n)y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Configuration saved to file: /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Configs to access meta-llama-3.2 model\n",
        "if \"HF_TOKEN\" not in os.environ:\n",
        "  os.environ[\"HF_TOKEN\"] = getpass(\"Enter your Hugging Face Key: \")"
      ],
      "metadata": {
        "id": "XPErrxnjLtqf",
        "outputId": "09820eb6-5ec3-4b72-f416-a79758d7a0c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI configuration (ignore if you're using LiteLLM)\n",
        "#if \"OPENAI_API_KEY\" not in os.environ:\n",
        "#    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "# MODEL = \"gpt-4o-mini\""
      ],
      "metadata": {
        "id": "cffXwutl1PBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Opik client and Model\n",
        "MODEL = \"huggingface/meta-llama/Llama-3.2-1B-Instruct\"\n",
        "client = Opik()"
      ],
      "metadata": {
        "id": "8g_BBxJz0zzj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts & Templates"
      ],
      "metadata": {
        "id": "hrSbwEqA14y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Use the following context to answer my question:\n",
        "\n",
        "### CONTEXT:\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Ch0EkZW317Qk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "kG5cyliF1G6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "dataset = client.get_or_create_dataset(\n",
        "    name=\"HaluBench\", description=\"HaluBench dataset\"\n",
        ")"
      ],
      "metadata": {
        "id": "URvNSIYq20Vq",
        "outputId": "e2a7889e-631c-466b-81ef-89a14026b736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Created a \"HaluBench\" dataset at https://www.comet.com/opik/bluemusk/redirect/datasets?name=HaluBench.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(\n",
        "    \"hf://datasets/PatronusAI/HaluBench/data/test-00000-of-00001.parquet\"\n",
        ")"
      ],
      "metadata": {
        "id": "SiatWxGI3NCy",
        "outputId": "5b1194e3-81fe-4f48-e2f3-7373534e1d5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "BK_DY_5_3T0N",
        "outputId": "d0f8ccf6-2064-4184-c5e1-340188137c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     id  \\\n",
              "0  d3fb4c3c-d21b-480a-baa0-98d6d0d17c1d   \n",
              "1  8603663e-c53b-46db-a482-a867f12ff3b4   \n",
              "2  c63a73e5-2c91-489b-bd24-af150ddfa82c   \n",
              "3  52db14ed-5426-46ec-b0ae-4ef843b2d692   \n",
              "4  31b36417-aad1-412c-b0e5-9c1faaed233f   \n",
              "\n",
              "                                             passage  \\\n",
              "0  Hoping to rebound from the road loss to the Ch...   \n",
              "1  As of the census of 2000, there were 218,590 p...   \n",
              "2  Hoping to rebound from the road loss to the Ch...   \n",
              "3  Hoping to rebound from their tough overtime ro...   \n",
              "4  As of the census of 2000, there were 218,590 p...   \n",
              "\n",
              "                                            question  \\\n",
              "0  Which team scored the longest field goal kick ...   \n",
              "1                  How many percent were not  Irish?   \n",
              "2  How many yards was the second longest field go...   \n",
              "3                   How long was the last touchdown?   \n",
              "4  How many in percent from the census weren't Ir...   \n",
              "\n",
              "                                              answer label source_ds  \n",
              "0  ['Rams', 'second', 'Marc Bulger', 'Kevin Curtis']  FAIL      DROP  \n",
              "1                                               87.1  FAIL      DROP  \n",
              "2                                                 42  FAIL      DROP  \n",
              "3                                            18-yard  FAIL      DROP  \n",
              "4                                               87.1  FAIL      DROP  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1989815d-a679-496f-9099-a9dc667c01c8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>label</th>\n",
              "      <th>source_ds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d3fb4c3c-d21b-480a-baa0-98d6d0d17c1d</td>\n",
              "      <td>Hoping to rebound from the road loss to the Ch...</td>\n",
              "      <td>Which team scored the longest field goal kick ...</td>\n",
              "      <td>['Rams', 'second', 'Marc Bulger', 'Kevin Curtis']</td>\n",
              "      <td>FAIL</td>\n",
              "      <td>DROP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8603663e-c53b-46db-a482-a867f12ff3b4</td>\n",
              "      <td>As of the census of 2000, there were 218,590 p...</td>\n",
              "      <td>How many percent were not  Irish?</td>\n",
              "      <td>87.1</td>\n",
              "      <td>FAIL</td>\n",
              "      <td>DROP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c63a73e5-2c91-489b-bd24-af150ddfa82c</td>\n",
              "      <td>Hoping to rebound from the road loss to the Ch...</td>\n",
              "      <td>How many yards was the second longest field go...</td>\n",
              "      <td>42</td>\n",
              "      <td>FAIL</td>\n",
              "      <td>DROP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>52db14ed-5426-46ec-b0ae-4ef843b2d692</td>\n",
              "      <td>Hoping to rebound from their tough overtime ro...</td>\n",
              "      <td>How long was the last touchdown?</td>\n",
              "      <td>18-yard</td>\n",
              "      <td>FAIL</td>\n",
              "      <td>DROP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31b36417-aad1-412c-b0e5-9c1faaed233f</td>\n",
              "      <td>As of the census of 2000, there were 218,590 p...</td>\n",
              "      <td>How many in percent from the census weren't Ir...</td>\n",
              "      <td>87.1</td>\n",
              "      <td>FAIL</td>\n",
              "      <td>DROP</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1989815d-a679-496f-9099-a9dc667c01c8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1989815d-a679-496f-9099-a9dc667c01c8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1989815d-a679-496f-9099-a9dc667c01c8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8bfab96f-677d-4d47-b0a0-cbf942033658\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8bfab96f-677d-4d47-b0a0-cbf942033658')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8bfab96f-677d-4d47-b0a0-cbf942033658 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 14900,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14900,\n        \"samples\": [\n          \"halueval-4865\",\n          \"halueval-5258\",\n          \"halueval-4193\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"passage\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12419,\n        \"samples\": [\n          \"Rhapidophyllum hystrix, the needle palm, is a palm, the sole member of the genus Rhapidophyllum. It is native to the subtropical southeastern United States. Endemic populations can be found from coastal southeast South Carolina, southward to Florida and west across the coastal plain of Mississippi and southern Alabama. It is one of the most cold-hardy palms in the world, and can be found growing in several areas with warm temperate climates.Spartium junceum, commonly known as Spanish broom or weaver's broom, is a species of flowering plant in the family Fabaceae.\",\n          \"The Pass is a 2016 film starring Russell Tovey and Arinze Kene. His other notable roles include Rudge in both the stage and film versions of \\\"The History Boys\\\", Steve in the BBC Three sitcom \\\"Him & Her\\\", Kevin Matheson in the HBO original series \\\"Looking\\\" and as Henry Knight on BBC TV series \\\"Sherlock\\\".\",\n          \"The failure of cancer treatments is partly due to the enrichment of cancer stem-like cells (CSLCs) that are resistant to conventional chemotherapy. A novel micelle formulation of oxaliplatin (OXA) encapsulated in chitosan vesicle was developed. The authors postulate that micelle encapsulation of OXA would eliminate both CSLCs and bulk cancer cells in colorectal cancer (CRC).\\nIn this study, using stearic acid-g-chitosan oligosaccharide (CSO-SA) polymeric micelles as a drug-delivery system, OXA-loaded CSO-SA micelles (CSO-SA/OXA) were prepared. Intracellular uptake of CSO-SA/OXA micelles was assessed by confocal microscope. The effects of free OXA, the empty carrier, and CSO-SA/OXA micelles were tested using human CRC cell lines in vitro and in vivo.\\nThe micelles showed excellent internalization ability that increased OXA accumulation both in CRC cells and tissues. Furthermore, CSO-SA/OXA micelles could either increase the cytotoxicity of OXA against the bulk cancer cells or reverse chemoresistance of CSLC subpopulations in vitro. Intravenous administration of CSO-SA/OXA micelles effectively suppressed the tumor growth and reduced CD133+/CD24+ cell (putative CRC CSLC markers) compared with free OXA treatment, which caused CSLC enrichment in xenograft tumors (P < 0.05).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13798,\n        \"samples\": [\n          \"In which county was the inventor of the compound bow born ?\",\n          \"Who else founded the Brooklyn Brewery with Tom Potter?\",\n          \"What sport did Robert Lindstedt and Alex O'Brien play?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13543,\n        \"samples\": [\n          \"Yes. Transvaginal Doppler examination can detect hemodynamic changes in uteroplacental circulation associated with subsequent adverse pregnancy outcome.\",\n          \"Vincent Walker is younger than David Usher.\",\n          \"Fort Benning, Georgia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"PASS\",\n          \"FAIL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_ds\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"DROP\",\n          \"pubmedQA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_ds = df.drop(['answer', 'label', 'source_ds', 'id'], axis=1).iloc[0:100]\n",
        "cleaned_ds.head()"
      ],
      "metadata": {
        "id": "cGs9V96R5B4f",
        "outputId": "1de3a90a-bb70-464d-982f-82291e5b7931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             passage  \\\n",
              "0  Hoping to rebound from the road loss to the Ch...   \n",
              "1  As of the census of 2000, there were 218,590 p...   \n",
              "2  Hoping to rebound from the road loss to the Ch...   \n",
              "3  Hoping to rebound from their tough overtime ro...   \n",
              "4  As of the census of 2000, there were 218,590 p...   \n",
              "\n",
              "                                            question  \n",
              "0  Which team scored the longest field goal kick ...  \n",
              "1                  How many percent were not  Irish?  \n",
              "2  How many yards was the second longest field go...  \n",
              "3                   How long was the last touchdown?  \n",
              "4  How many in percent from the census weren't Ir...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2067002e-2f42-4897-a83e-7630f5228d43\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hoping to rebound from the road loss to the Ch...</td>\n",
              "      <td>Which team scored the longest field goal kick ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>As of the census of 2000, there were 218,590 p...</td>\n",
              "      <td>How many percent were not  Irish?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hoping to rebound from the road loss to the Ch...</td>\n",
              "      <td>How many yards was the second longest field go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hoping to rebound from their tough overtime ro...</td>\n",
              "      <td>How long was the last touchdown?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>As of the census of 2000, there were 218,590 p...</td>\n",
              "      <td>How many in percent from the census weren't Ir...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2067002e-2f42-4897-a83e-7630f5228d43')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2067002e-2f42-4897-a83e-7630f5228d43 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2067002e-2f42-4897-a83e-7630f5228d43');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-86453cb7-24dc-40b2-bd6f-d6be91c99543\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-86453cb7-24dc-40b2-bd6f-d6be91c99543')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-86453cb7-24dc-40b2-bd6f-d6be91c99543 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "cleaned_ds",
              "summary": "{\n  \"name\": \"cleaned_ds\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"passage\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"Trying to rebound from back-to-back home losses, the Patriots traveled to Lambeau Field for a Week 11 showdown with the Green Bay Packers.  In the first quarter, Brady hooked up with Graham on a two-yard touchdown pass for the only score of the period.  In the second quarter, Dillon got a one-yard touchdown run, while Brady hooked up with Caldwell on a 54-yard touchdown pass.  Late in the quarter Packers quarterback Brett Favre was sacked by Tedy Bruschi and was knocked out of the game with injury to his right (throwing) wrist.  In the third quarter, Brady threw an eight-yard touchdown pass to Watson for the only score of the period.  In the fourth quarter, New England put the game away with Brady throwing his fourth touchdown pass of the day to Maroney, a 19-yard strike.  With the win, the Patriots improved to 7-3.\",\n          \"The Panthers started off their season by making their first return to Levi's Stadium since losing to the Denver Broncos in Super Bowl 50. Late in the first quarter Cam Newton threw a 40-yard touchdown to Russell Sheppard followed by a Graham Gano field goal. The Panthers scored six more points in the second quarter with two field goals. In the third Jonathan Stewart scored a touchdown, followed by another Gano field goal. With 3:14 left to go in the third quarter, Gano made his third field goal of the day making the score 23-0. Robbie Gould's kick with thirteen seconds to go gave the 49ers their first points of the game. Neither the Panthers or 49ers scored in the fourth quarter, resulting in Carolina defeating San Francisco 23-3. They improved to 1-0.\",\n          \"Hoping to rebound from the road loss to the Chargers, the Rams went home for Week 9, as they fought the Kansas City Chiefs in a \\\"Show Me State Showdown\\\". The Chiefs struck first as RB Larry Johnson got a 1-yard TD run for the only score of the period.  In the second quarter, things got worse for the Rams as QB Damon Huard completed a 3-yard TD pass to TE Tony Gonzalez, while kicker Lawrence Tynes nailed a 42-yard field goal.  St. Louis got on the board with RB Steven Jackson getting a 2-yard TD run, yet Huard and Gonzalez hooked up with each other again on a 25-yard TD strike.  Rams kicker Jeff Wilkins made a 41-yard field goal to end the half.  In the third quarter, QB Marc Bulger completed a 2-yard TD pass to WR Kevin Curtis for the only score of the period, yet the only score of the fourth quarter came from Huard completing an 11-yard TD pass to TE Kris Wilson. With the loss, the Rams fell to 4-4.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 96,\n        \"samples\": [\n          \"How many kickoff return touchdowns shorter than 100-yards did Leon Washington score?\",\n          \"How many in percent from the census weren't Polish?\",\n          \"Which team won the game?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.insert(cleaned_ds.to_dict('records'))"
      ],
      "metadata": {
        "id": "5qbQottd8d2-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the above dataset\n",
        "dataset.to_pandas().head()"
      ],
      "metadata": {
        "id": "vRxS4lkJNrZb",
        "outputId": "9c6bb6b3-d897-4b40-cded-d40e5b527bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             passage  \\\n",
              "0  Trying to snap a two-game skid, the Bills flew...   \n",
              "1  1564: The city of Ryazan posad was burned.:47 ...   \n",
              "2  As of the census of 2000, there were 218,590 p...   \n",
              "3  As of the census of 2000, there were 218,590 p...   \n",
              "4  In week 6, the Lions hosted the NFC West Divis...   \n",
              "\n",
              "                                            question  \\\n",
              "0  How many games had the Bills won before this g...   \n",
              "1  What was burned first: city of Ryazan or subur...   \n",
              "2                How many percent were not  Italian?   \n",
              "3  Which group from the census is smaller: German...   \n",
              "4  How many field goals between 20 and 30 yards w...   \n",
              "\n",
              "                                     id  \n",
              "0  01944236-a6fe-7de8-a55b-0993b307fbf4  \n",
              "1  01944236-a6fd-76bd-ba40-badafa549570  \n",
              "2  01944236-a6fc-7468-b0af-fd3aa8c87132  \n",
              "3  01944236-a6fb-7404-98fa-4af3ae6390f2  \n",
              "4  01944236-a6fa-715e-af4b-1131599ce150  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f6c29a6-838c-44cf-b04a-681490d8392f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passage</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Trying to snap a two-game skid, the Bills flew...</td>\n",
              "      <td>How many games had the Bills won before this g...</td>\n",
              "      <td>01944236-a6fe-7de8-a55b-0993b307fbf4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1564: The city of Ryazan posad was burned.:47 ...</td>\n",
              "      <td>What was burned first: city of Ryazan or subur...</td>\n",
              "      <td>01944236-a6fd-76bd-ba40-badafa549570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As of the census of 2000, there were 218,590 p...</td>\n",
              "      <td>How many percent were not  Italian?</td>\n",
              "      <td>01944236-a6fc-7468-b0af-fd3aa8c87132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>As of the census of 2000, there were 218,590 p...</td>\n",
              "      <td>Which group from the census is smaller: German...</td>\n",
              "      <td>01944236-a6fb-7404-98fa-4af3ae6390f2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In week 6, the Lions hosted the NFC West Divis...</td>\n",
              "      <td>How many field goals between 20 and 30 yards w...</td>\n",
              "      <td>01944236-a6fa-715e-af4b-1131599ce150</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f6c29a6-838c-44cf-b04a-681490d8392f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1f6c29a6-838c-44cf-b04a-681490d8392f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1f6c29a6-838c-44cf-b04a-681490d8392f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bf649b32-5431-4638-80a6-db019da943da\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bf649b32-5431-4638-80a6-db019da943da')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bf649b32-5431-4638-80a6-db019da943da button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"passage\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"1564: The city of Ryazan posad was burned.:47 1571: Russo-Crimean War 1572: Battle of Molodi 1591: Raid reaches Moscow. :116 1591: Artillery stops a raid at Kolomenskoy on the Bank Line. :52 1592: Suburbs of Moscow burned.  Russian troops were away fighting Sweden.:17 1598: Crimeans stopped by Bank Line, withdraw and sue for peace.:46 1614: Nogai raids within sight of Moscow. During the Time of Troubles so many captives were taken that the price of a slave at Kaffa dropped to fifteen or twenty gold pieces.:66 1618: Nogais release 15,000 captives in peace treaty with Moscow. 1632: Force from Livny ambushed by Tatars and Janissaries. 300 killed and the rest enslaved.:67 1632: 20,000 Tatars raid the south, as troops were shifted north for the Smolensk War.:76 1633: 30,000 Tatars cross Abatis and Bank lines. Thousands were captured from Oka region.:76 This was the last deep raid into Muscovy. :26 1635: Many small war parties invaded Russia south of Ryazan.:79 1637,41-43: Several raids were led by Nogais and Crimean nobles without permission of Khan.:90 1643: 600 Tatars and 200 Zaporozhian Cossacks raid Kozlov. 19 were killed, and 262 were captured.:23 1644: 20,000 The Tatars raid southern Muscovy, 10,000 captives.:91 1645: A raid captures 6,000 captives. It is claimed that the Turks encouraged these raids to obtain galley slaves for a war with Venice.:91\",\n          \"In week 6, the Lions hosted the NFC West Division-leading San Francisco 49ers. The Lions struck first, when Jason Hanson kicked a 25-yard field goal that came after a San Francisco turnover. Detroit added to their lead when Brandon Pettigrew caught a 16-yard TD pass. In the second quarter, the 49ers finally got on the board with a 1-yard TD run by Frank Gore. San Francisco received a safety when Aldon Smith sacked Matthew Stafford in the end zone. The final points of the first half came just before intermission when David Akers of the 49ers kicked a 55-yard field goal, putting the Lions down at halftime for the fourth consecutive week. After the break, Detroit scored on a 24-yard field goal. San Francisco responded with a field goal of their own, this one from 31 yards out. In the fourth quarter, the Lions' Nate Burleson caught a 5-yard TD pass; the Lions went for a 2-point conversion but Calvin Johnson failed to catch Matthew Stafford's pass. In the final 2 minutes, the 49ers rallied from behind, first with a 6-yard TD catch by Delanie Walker, then a 37-yard field goal by David Akers, putting them up 25-19. With 1:02 left in the game, the Lions attempted another comeback, but San Francisco's defense held them off, handing Detroit its first loss of the season as the team fell to 5-1 and 2nd place in the NFC North. After the game, coaches Jim Schwartz and Jim Harbaugh got into an argument after Schwartz claimed Harbaugh pushed him out of the way when the two coaches were shaking hands.\",\n          \"Trying to snap a two-game skid, the Bills flew to Gillette Stadium for a Week 3 divisional fight with the New England Patriots.  In the first quarter, QB J. P. Losman was immediately injured on the first offensive play of the game.  He would finish the series, but ended up on the bench for the rest of the game.  After New England took the lead with kicker Stephen Gostkowski's 24-yard field goal, rookie QB Trent Edwards played the rest of the game for Buffalo.  The Bills would get their only score of the game as RB Marshawn Lynch got an 8-yard TD run, and a Rian Lindell extra point put the Bills ahead surprisingly 7-3.  However, in the second quarter, the Patriots were able to open up their running game when Bills rookie standout Paul Posluszny was lost due to a broken arm. This left passing lanes open, and for the rest of the game, the Patriots dominated. QB Tom Brady's 8-yard TD pass to TE Benjamin Watson and a 3-yard TD pass to WR Randy Moss made it 17-7 at the half.  In the third quarter, New England continued its conquest with Brady's 4-yard TD pass to WR Jabar Gaffney and RB Sammy Morris' 4-yard TD run.  In the fourth quarter, the Patriots ended the day with Brady and Moss hooking up with each other again on a 45-yard TD pass.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"What was burned first: city of Ryazan or suburbs of Moscow?\",\n          \"How many field goals between 20 and 30 yards were made?\",\n          \"How many percent were not  Italian?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"01944236-a6fd-76bd-ba40-badafa549570\",\n          \"01944236-a6fa-715e-af4b-1131599ce150\",\n          \"01944236-a6fc-7468-b0af-fd3aa8c87132\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Application"
      ],
      "metadata": {
        "id": "oOMXQEh_01_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment function call of llama3 using litellm\n",
        "@track\n",
        "def chatbot_application(question: str, context: str) -> str:\n",
        "    response = litellm.completion(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "            {\"role\":\"user\", \"content\":prompt_template.format(context=context, question=question)}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "JZ8zFNx5Oe_P"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* below codes is for OpenAI and Lite LLM combo"
      ],
      "metadata": {
        "id": "57oxnj6PQDRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple little client class for using different LLM APIs (OpenAI or LiteLLM)\n",
        "#class LLMClient:\n",
        "#  def __init__(self, client_type: str =\"openai\", model: str =\"gpt-4\"):\n",
        "#    self.client_type = client_type\n",
        "#    self.model = model\n",
        "\n",
        "#    if self.client_type == \"openai\":\n",
        "#      self.client = track_openai(openai.OpenAI())\n",
        "\n",
        "#    else:\n",
        "#      self.client = None\n",
        "\n",
        "  # LiteLLM query function\n",
        "#  def _get_litellm_response(self, query: str, system: str = \"You are a helpful assistant.\"):\n",
        "#    messages = [\n",
        "#        {\"role\": \"system\", \"content\": system },\n",
        "#        { \"role\": \"user\", \"content\": query }\n",
        "#    ]\n",
        "\n",
        "#    response = litellm.completion(\n",
        "#        model=self.model,\n",
        "#        messages=messages\n",
        "#    )\n",
        "\n",
        "#    return response.choices[0].message.content\n",
        "\n",
        "  # OpenAI query function - use **kwargs to pass arguments like temperature\n",
        "#  def _get_openai_response(self, query: str, system: str = \"You are a helpful assistant.\", **kwargs):\n",
        "#    messages = [\n",
        "#        {\"role\": \"system\", \"content\": system },\n",
        "#        { \"role\": \"user\", \"content\": query }\n",
        "#    ]\n",
        "\n",
        "#    response = self.client.chat.completions.create(\n",
        "#        model=self.model,\n",
        "#        messages=messages,\n",
        "#        **kwargs\n",
        "#    )\n",
        "\n",
        "#    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "#  def query(self, query: str, system: str = \"You are a helpful assistant.\", **kwargs):\n",
        "#    if self.client_type == 'openai':\n",
        "#      return self._get_openai_response(query, system, **kwargs)\n",
        "\n",
        "#    else:\n",
        "#      return self._get_litellm_response(query, system)\n",
        "\n",
        "\n",
        "# llm_client = LLMClient(model=MODEL)\n",
        "\n",
        "\n",
        "@track\n",
        "#def chatbot_application(question: str, context: str) -> str:\n",
        "#    response = llm_client.query(prompt_template.format(context=context, question=question))\n",
        "#    return response\n"
      ],
      "metadata": {
        "id": "6pElcVpW1qUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1K928HHl1qLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA2XRQDQzY2T"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CXoa_D87zY2T"
      },
      "outputs": [],
      "source": [
        "# Define the evaluation task\n",
        "def evaluation_task(x):\n",
        "    return {\n",
        "        \"input\": x['question'],\n",
        "        \"output\": chatbot_application(x['question'], x['passage']),\n",
        "        \"context\": x['passage']\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the dataset\n",
        "client = Opik()"
      ],
      "metadata": {
        "id": "XlQ-dA9A7SpC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the metrics\n",
        "metrics = [Hallucination(), AnswerRelevance()]\n",
        "\n",
        "# experiment_name\n",
        "experiment_name = MODEL + \"_\" + dataset.name + \"_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "# run evaluation\n",
        "evaluation = evaluate(\n",
        "    experiment_name=experiment_name,\n",
        "    dataset=dataset,\n",
        "    task=evaluation_task,\n",
        "    scoring_metrics=metrics,\n",
        "    experiment_config={\n",
        "        \"model\": MODEL\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "LW7YCZI67T7k",
        "outputId": "31237083-d1ca-4d2b-dea9-20f04ea662b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c36066ea183d4cd1aada1737e04aa5ce",
            "556f5b926a8f4b70bedf512c3a100292",
            "e7ae511f8b2148439b7b011f5719c832",
            "23f7f543c45b404aa96a9370f898255f",
            "6307901142c24e7cb43ac40e0a7d8726",
            "30bb7b05812a45adb4cdbf13b4a7ef18",
            "965d7d4dc9884ee8aaf98f21efa843ca",
            "53bdd639f7704e62bc26781d1b4d0842",
            "3e2eb4f83be0497b89f178fdeaac189a",
            "d38d6b54c4b34e0dbc9f544929fc86b5",
            "7db642d6c1c348a5b3e5367992f24372"
          ]
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluation:   0%|          | 0/96 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c36066ea183d4cd1aada1737e04aa5ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:44 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:44 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:44 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Evaluation:  71%|███████   | 68/96 [00:20<00:02,  9.73it/s]ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:44 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:44 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:44 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Evaluation:  73%|███████▎  | 70/96 [00:20<00:02, 10.80it/s]ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "Evaluation:  75%|███████▌  | 72/96 [00:20<00:01, 12.14it/s]ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "Evaluation:  77%|███████▋  | 74/96 [00:20<00:01, 13.28it/s]\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Evaluation:  79%|███████▉  | 76/96 [00:21<00:01, 12.31it/s]OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Evaluation:  81%|████████▏ | 78/96 [00:21<00:01, 10.93it/s]OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:45 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "Evaluation:  83%|████████▎ | 80/96 [00:21<00:01, 10.41it/s]ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "Evaluation:  85%|████████▌ | 82/96 [00:21<00:01,  9.34it/s]ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "Evaluation:  90%|████████▉ | 86/96 [00:22<00:00, 11.40it/s]ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "Evaluation:  92%|█████████▏| 88/96 [00:22<00:00, 12.57it/s]ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Evaluation:  94%|█████████▍| 90/96 [00:22<00:00, 13.47it/s]OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:46 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "OPIK: Failed to compute metric hallucination_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/hallucination/metric.py\", line 94, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:47 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:47 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Evaluation:  96%|█████████▌| 92/96 [00:22<00:00, 10.25it/s]OPIK: Failed to compute metric answer_relevance_metric. Score result will be marked as failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 691, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 595, in completion\n",
            "    openai_client: OpenAI = self._get_openai_client(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 361, in _get_openai_client\n",
            "    _new_client = OpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_client.py\", line 101, in __init__\n",
            "    raise OpenAIError(\n",
            "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1567, in completion\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 1540, in completion\n",
            "    response = openai_chat_completions.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/openai/openai.py\", line 701, in completion\n",
            "    raise OpenAIError(\n",
            "litellm.llms.openai.common_utils.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/scorer.py\", line 37, in _score_test_case\n",
            "    result = metric.score(**score_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 173, in wrapper\n",
            "    raise exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/decorator/base_track_decorator.py\", line 163, in wrapper\n",
            "    result = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/metrics/llm_judges/answer_relevance/metric.py\", line 90, in score\n",
            "    model_output = self._model.generate_string(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 123, in generate_string\n",
            "    response = self.generate_provider_response(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/opik/evaluation/models/litellm_chat_model.py\", line 150, in generate_provider_response\n",
            "    response = self._engine.completion(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 998, in wrapper\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/utils.py\", line 876, in wrapper\n",
            "    result = original_function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/main.py\", line 2958, in completion\n",
            "    raise exception_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2189, in exception_type\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 355, in exception_type\n",
            "    raise AuthenticationError(\n",
            "litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:47 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:47 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Evaluation: 100%|██████████| 96/96 [00:22<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─ HaluBench (96 samples) ────────────────────────╮\n",
              "│                                                 │\n",
              "│ \u001b[1mTotal time:       \u001b[0m 00:00:23                     │\n",
              "│ \u001b[1mNumber of samples:\u001b[0m 96                           │\n",
              "│                                                 │\n",
              "│ \u001b[1;32mhallucination_metric: None (avg)\u001b[0m\u001b[31m - 96 failed\u001b[0m    │\n",
              "│ \u001b[1;32manswer_relevance_metric: None (avg)\u001b[0m\u001b[31m - 96 failed\u001b[0m │\n",
              "│                                                 │\n",
              "╰─────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ HaluBench (96 samples) ────────────────────────╮\n",
              "│                                                 │\n",
              "│ <span style=\"font-weight: bold\">Total time:       </span> 00:00:23                     │\n",
              "│ <span style=\"font-weight: bold\">Number of samples:</span> 96                           │\n",
              "│                                                 │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">hallucination_metric: None (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 96 failed</span>    │\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">answer_relevance_metric: None (avg)</span><span style=\"color: #800000; text-decoration-color: #800000\"> - 96 failed</span> │\n",
              "│                                                 │\n",
              "╰─────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92m19:36:47 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:47 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading results to Opik \u001b[33m...\u001b[0m \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "\u001b[92m19:36:47 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "View the results \u001b]8;id=171205;https://www.comet.com/opik/bluemusk/experiments/01944234-b980-73d2-b0ca-2e6acece1b92/compare?experiments=%5B%2201944244-a7bb-7bd2-b103-58956513efc8%22%5D\u001b\\in your Opik dashboard\u001b]8;;\u001b\\.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">View the results <a href=\"https://www.comet.com/opik/bluemusk/experiments/01944234-b980-73d2-b0ca-2e6acece1b92/compare?experiments=%5B%2201944244-a7bb-7bd2-b103-58956513efc8%22%5D\" target=\"_blank\">in your Opik dashboard</a>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# debugging why above experiment failed\n",
        "import httpx\n",
        "\n",
        "response = httpx.post(\n",
        "    \"https://www.comet.com/opik/api/v1/private/traces/batch\",\n",
        "    headers={\"Authorization\": \"Bearer YOUR_API_KEY\"},\n",
        "    json={\"your_payload\": \"data\"}\n",
        ")\n",
        "print(response.status_code, response.text)\n"
      ],
      "metadata": {
        "id": "S3ozbQJZ7SkP",
        "outputId": "42923619-5f59-44dc-ddf9-bc35aee2b391",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "403 {\"code\":403,\"message\":\"User not allowed to access workspace\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import httpx\n",
        "\n",
        "api_key = os.environ[\"OPIK_API_KEY\"]\n",
        "payload = {\"your_payload\": \"data\"}\n",
        "\n",
        "print(\"API Key:\", api_key)\n",
        "print(\"Payload:\", payload)\n",
        "\n",
        "response = httpx.post(\n",
        "    \"https://www.comet.com/opik/api/v1/private/traces/batch\",\n",
        "    headers={\"Authorization\": f\"Bearer {api_key}\"},\n",
        "    json=payload\n",
        ")\n",
        "print(\"Response Status Code:\", response.status_code)\n",
        "print(\"Response Text:\", response.text)\n"
      ],
      "metadata": {
        "id": "HtjC569w55mP",
        "outputId": "33996fdc-a09f-410b-98a0-d7d86fcb6d09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key: wkP6mtUG4vmMZp8yU4uJSXQh6\n",
            "Payload: {'your_payload': 'data'}\n",
            "Response Status Code: 403\n",
            "Response Text: {\"code\":403,\"message\":\"User not allowed to access workspace\"}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "comet-eval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rkMMwPik0obY",
        "hrSbwEqA14y2",
        "kG5cyliF1G6y",
        "oOMXQEh_01_u",
        "jA2XRQDQzY2T"
      ],
      "name": "Opik8-evaluation-llm-based.ipynb"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c36066ea183d4cd1aada1737e04aa5ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_556f5b926a8f4b70bedf512c3a100292",
              "IPY_MODEL_e7ae511f8b2148439b7b011f5719c832",
              "IPY_MODEL_23f7f543c45b404aa96a9370f898255f"
            ],
            "layout": "IPY_MODEL_6307901142c24e7cb43ac40e0a7d8726"
          }
        },
        "556f5b926a8f4b70bedf512c3a100292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30bb7b05812a45adb4cdbf13b4a7ef18",
            "placeholder": "​",
            "style": "IPY_MODEL_965d7d4dc9884ee8aaf98f21efa843ca",
            "value": "tokenizer.json: 100%"
          }
        },
        "e7ae511f8b2148439b7b011f5719c832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53bdd639f7704e62bc26781d1b4d0842",
            "max": 9084490,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e2eb4f83be0497b89f178fdeaac189a",
            "value": 9084490
          }
        },
        "23f7f543c45b404aa96a9370f898255f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d38d6b54c4b34e0dbc9f544929fc86b5",
            "placeholder": "​",
            "style": "IPY_MODEL_7db642d6c1c348a5b3e5367992f24372",
            "value": " 9.08M/9.08M [00:00&lt;00:00, 21.1MB/s]"
          }
        },
        "6307901142c24e7cb43ac40e0a7d8726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30bb7b05812a45adb4cdbf13b4a7ef18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965d7d4dc9884ee8aaf98f21efa843ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53bdd639f7704e62bc26781d1b4d0842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e2eb4f83be0497b89f178fdeaac189a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d38d6b54c4b34e0dbc9f544929fc86b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7db642d6c1c348a5b3e5367992f24372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}