{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\" width=\"250\"/>"
      ],
      "metadata": {
        "id": "ALLVb0Gl10XR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L95CZYXbKMT"
      },
      "source": [
        "# Logging Traces with the Open AI Integration\n",
        "\n",
        "In this exercise, you'll log some basic traces with Opik. You can use OpenAI or open source models via LiteLLM. You can find [the full documentation for Opik's integration with OpenAI here](https://www.comet.com/docs/opik/tracing/integrations/openai). You can find [the full documentation for Opik's integration with LiteLLM here](https://www.comet.com/docs/opik/cookbook/litellm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZC2yFjVbKMV"
      },
      "source": [
        "# Imports & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "26FJE2wtbKMU",
        "outputId": "0bc4f6d4-4781-4060-b832-2bcfd25f0bb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/297.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u001b[0m\u001b[91m\u001b[0m \u001b[32m297.0/297.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install opik openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opik\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Define your OPIK project name to enable tracing\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"logging_traces_demo\""
      ],
      "metadata": {
        "id": "a-YtJBxp87ap"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure Opik to have access\n",
        "if \"OPIK_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPIK_API_KEY\"] = getpass.getpass(\"Enter your Opik API key \")\n",
        "\n",
        "opik.configure()"
      ],
      "metadata": {
        "id": "vLLeF-Fa9Mai",
        "outputId": "5d13da84-7828-494d-8523-60ab39b37fc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Opik API key 路路路路路路路路路路\n",
            "Do you want to use \"bluemusk\" workspace? (Y/n)Y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Configuration saved to file: /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Below is necessary when using OpenAI."
      ],
      "metadata": {
        "id": "bIBGKac6--XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#if \"OPENAI_API_KEY\" not in os.environ:\n",
        "#    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "QovBOUFNqZF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zWLzUfibKMW"
      },
      "source": [
        "# Tracking OpenAI Calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbG-lxs-bKMW"
      },
      "outputs": [],
      "source": [
        "from opik.integrations.openai import track_openai\n",
        "from openai import OpenAI\n",
        "\n",
        "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "openai_client = track_openai(openai_client)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Hello, world!\"\n",
        "\n",
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "      {\"role\":\"user\", \"content\":prompt}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=100,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "3hP2oybz9beN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Open Source Models With LiteLLM\n",
        "Opik also integrates with LiteLLM, which allows you to use free open-source models and supports LLM APIs from all of the major providers (Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq, etc.) using the OpenAI format. [See here for a full list of LLM providers supported by LiteLLM as well as how to call them.](https://docs.litellm.ai/docs/providers)\n",
        "\n",
        "In the following example we'll use Meta's `Llama-3.1-8B-Instruct` model hosted on the Hugging Face hub.\n",
        "\n",
        "**If you have already run the OpenAI code above, you will need to restart your kernel before running the following code**"
      ],
      "metadata": {
        "id": "yg8N6_8Abd-g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zvhvjuQBbKMW"
      },
      "outputs": [],
      "source": [
        "%pip install opik litellm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opik\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Define your OPIK project name to enable tracing\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"logging-traces-litellm\"\n",
        "\n",
        "if \"OPIK_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPIK_API_KEY\"] = getpass.getpass(\"Enter your Opik API key: \")\n",
        "\n",
        "opik.configure()"
      ],
      "metadata": {
        "id": "XMf1_TCHfMc1",
        "outputId": "8bcc8f90-6322-4711-caba-f54c030bab19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Opik API key: 路路路路路路路路路路\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Opik is already configured. You can check the settings by viewing the config file at /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iouHRlmJbKMX"
      },
      "outputs": [],
      "source": [
        "from litellm.integrations.opik.opik import OpikLogger\n",
        "from opik.opik_context import get_current_span_data\n",
        "from opik import track\n",
        "import litellm\n",
        "\n",
        "opik_logger = OpikLogger()\n",
        "# In order to log LiteLLM traces to Opik, you will need to set the Opik callback\n",
        "litellm.callbacks = [opik_logger]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "if \"HF_TOKEN\" not in os.environ:\n",
        "    os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face API key: \")"
      ],
      "metadata": {
        "id": "fdWUie6kroz_",
        "outputId": "c285da29-c3f5-4621-e9cd-8503f618bee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face API key: 路路路路路路路路路路\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "vL8Ze4ShbKMX",
        "outputId": "37ed9756-39fb-45a1-acc5-54ec2475393d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ahahah! Don't worry, I've got some advice for you!\n",
            "\n",
            "If you have a llama in your garden, here are a few things to consider:\n",
            "\n",
            "1. **Check if it's a harmless llama**: If it's a domesticated llama, it's likely to be friendly and harmless. However, if you're not sure, it's always best to err on the side of caution and keep a safe distance.\n",
            "2. **Make sure it's not a stray**: If the llama is not wearing a tag or identification, it might be a stray. You can try to contact local animal shelters or llama rescue organizations to see\n"
          ]
        }
      ],
      "source": [
        "messages = [{ \"content\": \"There's a llama in my garden  What should I do?\",\"role\": \"user\"}]\n",
        "\n",
        "response = litellm.completion(\n",
        "    model=\"huggingface/meta-llama/Llama-3.2-1B-Instruct\",   # I used this llama version bcos I have access to it\n",
        "    messages=messages,\n",
        "    temperature=0.5,\n",
        "    max_tokens=130,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Refresh Opik on Comet to view the created project(logging-traces-llm) for the above prompt and response."
      ],
      "metadata": {
        "id": "bPl4YI4xIeJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My own example below -- not in the course. review again cos e no work"
      ],
      "metadata": {
        "id": "SNZHgCYiPQcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your OPIK project name to enable tracing\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"soccer-trace\"\n",
        "\n",
        "if \"OPIK_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPIK_API_KEY\"] = getpass.getpass(\"Enter your Opik API key: \")\n",
        "\n",
        "opik.configure()"
      ],
      "metadata": {
        "id": "9a6mepvvWpPO",
        "outputId": "d0d34630-edb0-4c98-eb76-22fea354f682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Opik is already configured. You can check the settings by viewing the config file at /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [{\"content\":\"what country is Lionel Messi from\", \"role\":\"user\"}]\n",
        "\n",
        "response = litellm.completion(\n",
        "    model=\"huggingface/meta-llama/Llama-3.2-1B-Instruct\",   # I used this llama version bcos I have access to it\n",
        "    messages=messages,\n",
        "    temperature=0.5,\n",
        "    max_tokens=130,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "diTl2FklImBI",
        "outputId": "1be243da-fad3-45bb-d94f-1af428ebc8ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ahahah! Don't worry, I've got some advice for you!\n",
            "\n",
            "If you have a llama in your garden, here are a few things to consider:\n",
            "\n",
            "1. **Check if it's a harmless llama**: If it's a domesticated llama, it's likely to be friendly and harmless. However, if you're not sure, it's always best to err on the side of caution and keep a safe distance.\n",
            "2. **Make sure it's not a stray**: If the llama is not wearing a tag or identification, it might be a stray. You can try to contact local animal shelters or llama rescue organizations to see\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to set environment variable with API_key and .env\n",
        "\n",
        "#from dotenv import load_dotenv, find_dotenv\n",
        "#_ = load_dotenv(find_dotenv())\n",
        "\n",
        "# Define your OPIK project name to enable tracing\n",
        "#os.environ[\"OPIK_PROJECT_NAME\"] = \"logging-traces-litellm\"\n",
        "\n",
        "#os.environ['OPIK_API_KEY'] = os.getenv('api_key name for Opik in .env')\n",
        "#os.environ['HF_TOKEN'] = os.getenv('api_key name for Huggingface in .env')"
      ],
      "metadata": {
        "id": "IpgkZrUsUC2v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "comet-eval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rZC2yFjVbKMV",
        "5zWLzUfibKMW",
        "yg8N6_8Abd-g"
      ],
      "name": "Opik1-intro-tracing.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}