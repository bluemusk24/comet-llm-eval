{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\" width=\"250\"/>"
      ],
      "metadata": {
        "id": "JyRiHeF6_doD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAHuR_PSVMhP"
      },
      "source": [
        "# Build & Monitor a YouTube Search Assistant\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you're going to build a YouTube search assistant and implement monitoring with Opik. You can use OpenAI or LiteLLM for your LLM API. The basic architecture for your application looks like this:\n",
        "\n",
        "- Users submit a question\n",
        "- Your application searches YouTube for relevant videos\n",
        "- Your application uses SentenceTransformers to extract relevant information from the video transcripts\n",
        "- Finally, your application passes the relevant information + question to your LLM API and returns the answer to the user"
      ],
      "metadata": {
        "id": "03DjzjB8hDGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Configuration"
      ],
      "metadata": {
        "id": "FY3ONLhb_fPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install opik openai litellm pytube youtube-transcript-api sentence-transformers --quiet"
      ],
      "metadata": {
        "id": "cArXTpDBlaJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90019dab-e72a-4867-a262-d3822446307d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.7/304.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-udSJ-rHVMhT"
      },
      "outputs": [],
      "source": [
        "import opik\n",
        "from opik import Opik, track\n",
        "from opik.integrations.openai import track_openai\n",
        "import openai\n",
        "import os\n",
        "import litellm\n",
        "from getpass import getpass\n",
        "from litellm.integrations.opik.opik import OpikLogger\n",
        "from opik.opik_context import get_current_span_data\n",
        "\n",
        "opik_logger = OpikLogger()\n",
        "# In order to log LiteLLM traces to Opik, you will need to set the Opik callback\n",
        "litellm.callbacks = [opik_logger]\n",
        "\n",
        "os.environ[\"OPIK_PROJECT_NAME\"] = \"youtube_search_assistant\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Opik configuration\n",
        "if \"OPIK_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPIK_API_KEY\"] = getpass(\"Enter your Opik API key: \")\n",
        "\n",
        "opik.configure()"
      ],
      "metadata": {
        "id": "ot5-tWOXd8X0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0440b14-e367-43ca-a37b-bb7dbee5f8e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Opik API key: ··········\n",
            "Do you want to use \"bluemusk\" workspace? (Y/n)y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Configuration saved to file: /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI configuration (ignore if you're using LiteLLM)\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n"
      ],
      "metadata": {
        "id": "yyjDSp7uh3zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a0f6fe8-dbdd-4325-93c2-b202866d59a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Application"
      ],
      "metadata": {
        "id": "4xxz5mOv_joo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple little client class for using different LLM APIs (OpenAI or LiteLLM)\n",
        "class LLMClient:\n",
        "  def __init__(self, client_type: str =\"openai\", model: str =\"gpt-4\"):\n",
        "    self.client_type = client_type\n",
        "    self.model = model\n",
        "\n",
        "    if self.client_type == \"openai\":\n",
        "      self.client = track_openai(openai.OpenAI())\n",
        "\n",
        "    else:\n",
        "      self.client = None\n",
        "\n",
        "  # LiteLLM query function\n",
        "  def _get_litellm_response(self, query: str, system: str = \"You are a helpful assistant.\"):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system },\n",
        "        { \"role\": \"user\", \"content\": query }\n",
        "    ]\n",
        "\n",
        "    response = litellm.completion(\n",
        "        model=self.model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "  # OpenAI query function - use **kwargs to pass arguments like temperature\n",
        "  def _get_openai_response(self, query: str, system: str = \"You are a helpful assistant.\", **kwargs):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system },\n",
        "        { \"role\": \"user\", \"content\": query }\n",
        "    ]\n",
        "\n",
        "    response = self.client.chat.completions.create(\n",
        "        model=self.model,\n",
        "        messages=messages,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "  def query(self, query: str, system: str = \"You are a helpful assistant.\", **kwargs):\n",
        "    if self.client_type == 'openai':\n",
        "      return self._get_openai_response(query, system, **kwargs)\n",
        "\n",
        "    else:\n",
        "      return self._get_litellm_response(query, system)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PBIyVFjKeJJ5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Configs to access model\n",
        "if \"HF_TOKEN\" not in os.environ:\n",
        "  os.environ[\"HF_TOKEN\"] = getpass(\"Enter your Hugging Face Key: \")"
      ],
      "metadata": {
        "id": "JRVznlXKxL3A",
        "outputId": "3399c828-e7fc-4718-920d-1a41343fc2c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your client!\n",
        "\n",
        "client = LLMClient(client_type=\"litellm\", model=\"huggingface/meta-llama/Llama-3.2-3B-Instruct\")"
      ],
      "metadata": {
        "id": "stqRZOGwgVQV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import Search\n",
        "\n",
        "def search_youtube(query: str):\n",
        "    # Use PyTube's Search class to perform the search\n",
        "    search = Search(query)\n",
        "\n",
        "    # Get the first 5 video results\n",
        "    videos = search.results[:5]\n",
        "\n",
        "    # Extract the video URLs\n",
        "    video_urls = [f\"https://www.youtube.com/watch?v={video.video_id}\" for video in videos]\n",
        "\n",
        "    return video_urls\n"
      ],
      "metadata": {
        "id": "OPnT9he0a8Gt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "def get_video_transcripts(video_urls: list):\n",
        "    transcripts = []\n",
        "    for url in video_urls:\n",
        "        video_id = url.split(\"v=\")[1]\n",
        "        try:\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "            full_transcript = \" \".join([entry['text'] for entry in transcript])\n",
        "            transcripts.append(full_transcript)\n",
        "        except Exception as e:\n",
        "            transcripts.append(f\"Error retrieving transcript for {url}: {str(e)}\")\n",
        "\n",
        "    return transcripts\n"
      ],
      "metadata": {
        "id": "T0Hzup0qa8DW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def find_relevant_context(query: str, transcripts: list, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    query_embedding = model.encode([query])\n",
        "\n",
        "    best_match = \"\"\n",
        "    highest_similarity = -1\n",
        "    for transcript in transcripts:\n",
        "        transcript_embedding = model.encode([transcript])\n",
        "        similarity = cosine_similarity(query_embedding, transcript_embedding)[0][0]\n",
        "        if similarity > highest_similarity:\n",
        "            highest_similarity = similarity\n",
        "            best_match = transcript\n",
        "\n",
        "    return best_match\n"
      ],
      "metadata": {
        "id": "m5rCq_Mda8CG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@track\n",
        "def query_llm_with_context(query: str, context: str):\n",
        "    prompt = f\"Given the following context: {context}\\nAnswer the question: {query}\"\n",
        "\n",
        "    return client.query(prompt)\n"
      ],
      "metadata": {
        "id": "2ykve-j3a7_V"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise"
      ],
      "metadata": {
        "id": "h_3pEqkK_mS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise time! Try completing the missing sections in the below function:\n",
        "\n",
        "def question_answer_system(user_query: str):\n",
        "    # Step 1: Search YouTube with the phrase\n",
        "    video_urls = search_youtube(user_query)\n",
        "\n",
        "    # Step 2: Pull transcripts for the videos\n",
        "    transcripts = get_video_transcripts(video_urls)\n",
        "\n",
        "    # Step 3: Find relevant context\n",
        "    relevant_context = find_relevant_context(user_query, transcripts)\n",
        "\n",
        "    # Step 4: Query the LLM with the context\n",
        "    final_answer = query_llm_with_context(user_query, relevant_context)\n",
        "\n",
        "    return final_answer\n"
      ],
      "metadata": {
        "id": "ajJE8NL3a751"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test it out!\n",
        "\n",
        "user_questions = [\n",
        "    \"Who is Moo Deng?\",\n",
        "    \"What is Agentic AI\"\n",
        "    # Add your own questions\n",
        "]"
      ],
      "metadata": {
        "id": "d6x3Fe8Ca72z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question in user_questions:\n",
        "  answer = question_answer_system(question)\n",
        "  print(answer)"
      ],
      "metadata": {
        "id": "_4-eV77ea7qW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1c042bace963403f8ef7ee1e67044318",
            "91a7117d2f49451fb16b34000ec032f5",
            "82f5bbe1bbc149948032eccd55dbd5d0",
            "5b6a6ad3eeac4f668566a204cfc083ec",
            "eebf4d99d3424bdf9665bde0bf3d7d26",
            "6080763a553648cd9a1da984dada3be2",
            "dc3918e06b93498c9d0a27c3f3513c4c",
            "90afea6e63884384a3c1523123ce9627",
            "c2477bb77d3d412fb8f76329ce8441b6",
            "fa5fe2c2887046bba1b6866bacee071a",
            "7cf005078b0a48779f491b0c842f52af"
          ]
        },
        "outputId": "e79227e9-f046-4e2f-f1f7-19487050b9e6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pytube.contrib.search:Unexpected renderer encountered.\n",
            "WARNING:pytube.contrib.search:Renderer name: dict_keys(['reelShelfRenderer'])\n",
            "WARNING:pytube.contrib.search:Search term: Who is Moo Deng?\n",
            "WARNING:pytube.contrib.search:Please open an issue at https://github.com/pytube/pytube/issues and provide this log output.\n",
            "WARNING:pytube.contrib.search:Unexpected renderer encountered.\n",
            "WARNING:pytube.contrib.search:Renderer name: dict_keys(['reelShelfRenderer'])\n",
            "WARNING:pytube.contrib.search:Search term: Who is Moo Deng?\n",
            "WARNING:pytube.contrib.search:Please open an issue at https://github.com/pytube/pytube/issues and provide this log output.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c042bace963403f8ef7ee1e67044318"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92m04:34:57 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/traces/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moo Deng is a two-month-old pygmy hippo who has become an internet sensation due to her extreme cuteness.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92m04:34:57 - LiteLLM:ERROR\u001b[0m: opik.py:111 - OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "ERROR:LiteLLM:OpikLogger failed to send batch - Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/integrations/opik/opik.py\", line 102, in _sync_send\n",
            "    response = self.sync_httpx_client.post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 528, in post\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/litellm/llms/custom_httpx/http_handler.py\", line 509, in post\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/httpx/_models.py\", line 763, in raise_for_status\n",
            "    raise HTTPStatusError(message, request=request, response=self)\n",
            "httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.comet.com/opik/api/v1/private/spans/batch'\n",
            "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n",
            "WARNING:pytube.contrib.search:Unexpected renderer encountered.\n",
            "WARNING:pytube.contrib.search:Renderer name: dict_keys(['reelShelfRenderer'])\n",
            "WARNING:pytube.contrib.search:Search term: What is Agentic AI\n",
            "WARNING:pytube.contrib.search:Please open an issue at https://github.com/pytube/pytube/issues and provide this log output.\n",
            "WARNING:pytube.contrib.search:Unexpected renderer encountered.\n",
            "WARNING:pytube.contrib.search:Renderer name: dict_keys(['lockupViewModel'])\n",
            "WARNING:pytube.contrib.search:Search term: What is Agentic AI\n",
            "WARNING:pytube.contrib.search:Please open an issue at https://github.com/pytube/pytube/issues and provide this log output.\n",
            "WARNING:pytube.contrib.search:Unexpected renderer encountered.\n",
            "WARNING:pytube.contrib.search:Renderer name: dict_keys(['reelShelfRenderer'])\n",
            "WARNING:pytube.contrib.search:Search term: What is Agentic AI\n",
            "WARNING:pytube.contrib.search:Please open an issue at https://github.com/pytube/pytube/issues and provide this log output.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agentic AI refers to artificial intelligence systems designed to operate as autonomous agents, making decisions, taking actions, and interacting with their environment without continuous human intervention.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note: head to Project on OPIK to see created chain -- Project, user qustion and outcome"
      ],
      "metadata": {
        "id": "CHd-odgxzmsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implemented question_answer_system()"
      ],
      "metadata": {
        "id": "OcTnXvbqug9G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIsLRDhpVMhW"
      },
      "outputs": [],
      "source": [
        "def question_answer_system(user_query: str):\n",
        "    # Step 1: Search YouTube with the phrase\n",
        "    video_urls = search_youtube(user_query)\n",
        "\n",
        "    # Step 2: Pull transcripts for the videos\n",
        "    transcripts = get_video_transcripts(video_urls)\n",
        "\n",
        "    # Step 3: Find relevant context\n",
        "    relevant_context = find_relevant_context(user_query, transcripts)\n",
        "\n",
        "    # Step 4: Query the LLM with the context\n",
        "    final_answer = query_llm_with_context(user_query, relevant_context)\n",
        "\n",
        "    return final_answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GufQWF_CVMhW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLS1-c35VMhX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "comet-eval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FY3ONLhb_fPO",
        "4xxz5mOv_joo",
        "h_3pEqkK_mS5",
        "OcTnXvbqug9G"
      ],
      "name": "monitor-youtube-assistant.ipynb"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c042bace963403f8ef7ee1e67044318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91a7117d2f49451fb16b34000ec032f5",
              "IPY_MODEL_82f5bbe1bbc149948032eccd55dbd5d0",
              "IPY_MODEL_5b6a6ad3eeac4f668566a204cfc083ec"
            ],
            "layout": "IPY_MODEL_eebf4d99d3424bdf9665bde0bf3d7d26"
          }
        },
        "91a7117d2f49451fb16b34000ec032f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6080763a553648cd9a1da984dada3be2",
            "placeholder": "​",
            "style": "IPY_MODEL_dc3918e06b93498c9d0a27c3f3513c4c",
            "value": "tokenizer.json: 100%"
          }
        },
        "82f5bbe1bbc149948032eccd55dbd5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90afea6e63884384a3c1523123ce9627",
            "max": 9084490,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2477bb77d3d412fb8f76329ce8441b6",
            "value": 9084490
          }
        },
        "5b6a6ad3eeac4f668566a204cfc083ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa5fe2c2887046bba1b6866bacee071a",
            "placeholder": "​",
            "style": "IPY_MODEL_7cf005078b0a48779f491b0c842f52af",
            "value": " 9.08M/9.08M [00:00&lt;00:00, 36.5MB/s]"
          }
        },
        "eebf4d99d3424bdf9665bde0bf3d7d26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6080763a553648cd9a1da984dada3be2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc3918e06b93498c9d0a27c3f3513c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90afea6e63884384a3c1523123ce9627": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2477bb77d3d412fb8f76329ce8441b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa5fe2c2887046bba1b6866bacee071a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cf005078b0a48779f491b0c842f52af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}